{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Used for DL applications, computer vision related processes\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# For image preprocessing\n",
    "from torchvision import transforms\n",
    "\n",
    "# Combines dataset & sampler to provide iterable over the dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# To recognise face from extracted frames\n",
    "import face_recognition\n",
    "\n",
    "# Autograd: PyTorch package for differentiation of all operations on Tensors\n",
    "# Variable are wrappers around Tensors that allow easy automatic differentiation\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "# 'nn' Help us in creating & training of neural network\n",
    "from torch import nn\n",
    "\n",
    "# Contains definition for models for addressing different tasks i.e. image classification, object detection e.t.c.\n",
    "from torchvision import models\n",
    "\n",
    "from skimage import img_as_ubyte\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# UPLOAD_FOLDER = 'Uploaded_Files'\n",
    "# video_path = \"\"\n",
    "\n",
    "detectOutput = []\n",
    "\n",
    "\n",
    "# Creating Model Architecture\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, num_classes, latent_dim= 2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n",
    "    super(Model, self).__init__()\n",
    "\n",
    "    # returns a model pretrained on ImageNet dataset\n",
    "    model = models.resnext50_32x4d(pretrained= True)\n",
    "\n",
    "    # Sequential allows us to compose modules nn together\n",
    "    self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "    # RNN to an input sequence\n",
    "    self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n",
    "\n",
    "    # Activation function\n",
    "    self.relu = nn.LeakyReLU()\n",
    "\n",
    "    # Dropping out units (hidden & visible) from NN, to avoid overfitting\n",
    "    self.dp = nn.Dropout(0.4)\n",
    "\n",
    "    # A module that creates single layer feed forward network with n inputs and m outputs\n",
    "    self.linear1 = nn.Linear(2048, num_classes)\n",
    "\n",
    "    # Applies 2D average adaptive pooling over an input signal composed of several input planes\n",
    "    self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, seq_length, c, h, w = x.shape\n",
    "\n",
    "    # new view of array with same data\n",
    "    x = x.view(batch_size*seq_length, c, h, w)\n",
    "\n",
    "    fmap = self.model(x)\n",
    "    x = self.avgpool(fmap)\n",
    "    x = x.view(batch_size, seq_length, 2048)\n",
    "    x_lstm,_ = self.lstm(x, None)\n",
    "    return fmap, self.dp(self.linear1(x_lstm[:,-1,:]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "im_size = 112\n",
    "\n",
    "# std is used in conjunction with mean to summarize continuous data\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "\n",
    "# provides the measure of dispersion of image grey level intensities\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Often used as the last layer of a nn to produce the final output\n",
    "sm = nn.Softmax()\n",
    "\n",
    "# Normalising our dataset using mean and std\n",
    "inv_normalize = transforms.Normalize(mean=-1*np.divide(mean, std), std=np.divide([1,1,1], std))\n",
    "\n",
    "# For image manipulation\n",
    "def im_convert(tensor):\n",
    "  image = tensor.to(\"cpu\").clone().detach()\n",
    "  image = image.squeeze()\n",
    "  image = inv_normalize(image)\n",
    "  image = image.numpy()\n",
    "  image = image.transpose(1,2,0)\n",
    "  image = image.clip(0,1)\n",
    "  cv2.imwrite('./2.png', image*255)\n",
    "  return image\n",
    "\n",
    "# For prediction of output  \n",
    "def predict(model, img, path='./'):\n",
    "  fmap, logits = model(img.to())\n",
    "  params = list(model.parameters())\n",
    "  weight_softmax = model.linear1.weight.detach().cpu().numpy()\n",
    "  logits = sm(logits)\n",
    "  _, prediction = torch.max(logits, 1)\n",
    "  confidence = logits[:, int(prediction.item())].item()*100\n",
    "  print('confidence of prediction: ', logits[:, int(prediction.item())].item()*100)\n",
    "  return [int(prediction.item()), confidence, fmap]\n",
    "\n",
    "\n",
    "\n",
    "# To validate the dataset\n",
    "class validation_dataset(Dataset):\n",
    "  def __init__(self, video_names, sequence_length = 60, transform=None):\n",
    "    self.video_names = video_names\n",
    "    self.transform = transform\n",
    "    self.count = sequence_length\n",
    "\n",
    "  # To get number of videos\n",
    "  def __len__(self):\n",
    "    return len(self.video_names)\n",
    "\n",
    "  # To get number of frames\n",
    "  def __getitem__(self, idx):\n",
    "    video_path = self.video_names[idx]\n",
    "    frames = []\n",
    "    a = int(100 / self.count)\n",
    "    first_frame = np.random.randint(0,a)\n",
    "    for i, frame in enumerate(self.frame_extract(video_path)):\n",
    "      faces = face_recognition.face_locations(frame)\n",
    "      try:\n",
    "        top,right,bottom,left = faces[0]\n",
    "        frame = frame[top:bottom, left:right, :]\n",
    "      except:\n",
    "        pass\n",
    "      frames.append(self.transform(frame))\n",
    "      if(len(frames) == self.count):\n",
    "        break\n",
    "    frames = torch.stack(frames)\n",
    "    frames = frames[:self.count]\n",
    "    return frames.unsqueeze(0)\n",
    "\n",
    "  # To extract number of frames\n",
    "  def frame_extract(self, path):\n",
    "    vidObj = cv2.VideoCapture(path)\n",
    "    success = 1\n",
    "    while success:\n",
    "      success, image = vidObj.read()\n",
    "      if success:\n",
    "        yield image\n",
    "\n",
    "\n",
    "def detectFakeVideo(videoPath):\n",
    "    im_size = 112\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "                                        transforms.ToPILImage(),\n",
    "                                        transforms.Resize((im_size,im_size)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean,std)])\n",
    "    path_to_videos= [videoPath]\n",
    "\n",
    "    video_dataset = validation_dataset(path_to_videos,sequence_length = 20,transform = train_transforms)\n",
    "    # use this command for gpu\n",
    "    # model = Model(2).cuda()\n",
    "    model = Model(2)\n",
    "    path_to_model = 'df_model.pt'\n",
    "    model.load_state_dict(torch.load(path_to_model, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    for i in range(0, len(path_to_videos)):\n",
    "        print(path_to_videos[i])\n",
    "        prediction, confidence, fmap = predict(model, video_dataset[i], './')\n",
    "        if prediction == 1:\n",
    "           print(\"REAL\")\n",
    "        else:\n",
    "           print(\"FAKE\")\n",
    "    return prediction, confidence, fmap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "def visualize_feature_maps(fmap):\n",
    "    fmap = fmap.detach().cpu().numpy()\n",
    "    fmap = np.squeeze(fmap)\n",
    "\n",
    "    num_channels = fmap.shape[0]\n",
    "    grid_size = int(math.ceil(math.sqrt(num_channels)))\n",
    "\n",
    "    fig, ax = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            index = i * grid_size + j\n",
    "            if index < num_channels:\n",
    "                ax[i, j].imshow(fmap[index], cmap='gray')\n",
    "            ax[i, j].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Usage:\n",
    "import matplotlib.pyplot as plt\n",
    "video_filename = input(\"Enter the filename\")\n",
    "video_path = \"Uploaded_Files/\" + video_filename\n",
    "\n",
    "prediction, confidence, fmap = detectFakeVideo(video_path)\n",
    "visualize_feature_maps(fmap)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
